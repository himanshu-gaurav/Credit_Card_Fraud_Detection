# -*- coding: utf-8 -*-
"""PRML_MINOR_PROJECT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pQ0fHre6Tu2rphA-eC5eFqRSxZg_jAmU

# Importing the necessary libraries
"""

import numpy as np
import pandas as pd
import seaborn as sns
import plotly.express as px


from matplotlib import pyplot as plt
from sklearn.model_selection import cross_val_score

"""# Loading data from the Google Drive"""

from google.colab import drive
drive.mount('/content/drive')

"""Importing the Dataset using Pandas.read_csv"""

data = pd.read_csv('/content/drive/MyDrive/creditcard.csv')
data.head()

"""Getting necessary information about the Dataset"""

data.describe()

"""Finding the Correlation between the Features"""

data.corr()

"""Finding Number of Null values in Dataset"""

sns.heatmap(data.isnull(),yticklabels=False)

sns.heatmap(data.corr())

data.info()

"""# Looking at the distribution of the Features of the Dataset"""

numeric_columns = (list(data.loc[:, 'Time':'Amount']))


fig = plt.figure(figsize=(25, 10))
rows, cols = 5, 6
for idx, num in enumerate(numeric_columns[:30]):
    ax = fig.add_subplot(rows, cols, idx+1)
    ax.grid(alpha = 0.7, axis ="both")
    sns.kdeplot(x = num, fill = True,color ="#3386FF",linewidth=0.6, data = data)
    ax.set_xlabel(num)
    # ax.legend()
fig.tight_layout()
fig.show()

"""Making Pie Plot for Class Distribution"""

Class_count = data['Class'].value_counts()
plt.pie(Class_count, labels=Class_count.index, autopct='%1.1f%%', startangle=90)
plt.axis('equal')
plt.title('Percentage of Each Class Type')
plt.show()

"""Violen Plot for the Data"""

sns.violinplot(x=data['Class'], y=data['Amount'], palette=['blue', 'red'])
plt.show()

"""# Splitting the Dataset into X and Y

Also, removing Time since it is not a important feature for training


"""

Y = data['Class']
X = data.drop(['Class', 'Time'], axis = 1).copy()

"""Making the Testing Data"""

number_records_fraud = len (data[data.Class==1])

fraud_indices = np.array (data[data.Class==1].index)
normal_indices = np.array (data[data.Class==0].index)

random_normal_indices = np.random.choice (normal_indices, number_records_fraud, replace = False )
random_fraud_indices = np.random.choice (fraud_indices, number_records_fraud, replace = True )

under_sample_indices = np.concatenate ([random_fraud_indices, random_normal_indices])

print(under_sample_indices.shape)

X_test = X.iloc[under_sample_indices,:]
Y_test = Y.iloc[under_sample_indices]

import seaborn as sns
import matplotlib.pyplot as plt

def create_implots(df):

    features = [col for col in df.columns if col != "Amount"]

    for feature in features:
        sns.set_style("whitegrid")
        sns.lmplot(x="Amount", y=feature, data=df,hue="Class", height=3,aspect=1.5, scatter_kws={'alpha':0.3}, facet_kws=dict(sharex=True, sharey=True))
        plt.title(f"Implot of {feature} vs Amount")
        plt.show()



create_implots(data.iloc[under_sample_indices, :])

"""PCA reduces the feature space to 3 dimensions for the test dataset X test. Plotly builds a 3D scatter plot of modified data."""

from sklearn.decomposition import PCA
import plotly.graph_objects as go
import numpy as np


PCA = PCA(n_components = 3)

X1 = PCA.fit_transform(X_test)


fig = go.Figure(data=[go.Scatter3d(x=X1[:, 0], y=X1[:, 1], z=X1[:, 2],mode='markers',
                                  marker=dict(size=3, color=Y_test))])
fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))
fig.show()


# Blue represents Not-Fraud and Yellow represents Fraud entris



"""Exploring Data by applying Linear Discriminant Ananlysis (LDA)"""

class LDA:
    def __init__(self):
        self.idx = []

    def fit(self, X, Y, var = 0.95):

        self.X = X
        self.Y = Y
        unique_classes = np.unique(Y)

        class_data = []

        for c in unique_classes:
            class_data.append((X[Y == c]))

        class_means = []
        for temp_class in class_data:
            class_means.append(np.mean(temp_class, axis=0))

        class_means = np.array(class_means)

        grand_mean = np.mean(X, axis=0)

        self.Scatter_with = np.zeros((X.shape[1], X.shape[1]))

        for i, d in enumerate(class_data):
            diff = d - class_means[i]
            self.Scatter_with += np.dot(diff.T, diff)

        self.Scatter_btw = np.zeros((X.shape[1], X.shape[1]))

        for i, mean_vec in enumerate(class_means):
            n = class_data[i].shape[0]
            diff = (mean_vec - grand_mean).reshape(-1,1)
            self.Scatter_btw += n * np.dot(diff, diff.T)


        self.eigvalues, self.eigvectors = np.linalg.eig(np.dot(np.linalg.inv(self.Scatter_with), self.Scatter_btw))

        temp_zip = zip(self.eigvalues, self.eigvectors.T)

        self.temp_sorted_zip = sorted(temp_zip, key = lambda x: x[0], reverse = True)


    def transform(self, X, n):
        self.W_matrix = []
        for i in range(n):
            self.W_matrix.append(self.temp_sorted_zip[i][1])

        self.W_matrix = np.array(self.W_matrix)
        self.W_matrix = self.W_matrix.T.real

        return np.dot(X, self.W_matrix).real

    def scatter_within(self):
        return self.Scatter_with
    def scatter_between(self):
        return self.Scatter_btw

LD = LDA()
LD.fit(np.array(X_test), np.array(Y_test))
X_new = LD.transform(X_test, 3)
X_new.shape

import plotly.graph_objects as go


fig = go.Figure(data=[go.Scatter3d(x=X_new[:, 0], y=X_new[:, 1], z=X_new[:, 2],mode='markers',
                                  marker=dict(size=3, color=Y_test))])
fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))
fig.show()





"""Making Model from Scratch"""

from sklearn.ensemble import VotingClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier as KNN
from sklearn.cluster import KMeans
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier



class BaggingClassifier:

    def __init__(self, clf = DecisionTreeClassifier(), n_estimators = 30):
        self.model = clf
        self.n_estimators = n_estimators
        self.classifiers = []


    def fit(self, X, Y):
        for i in range(self.n_estimators):

            no_frauds = len(Y[Y==1])
            indices_fraud = list(Y[Y==1].index)
            indices_normal = list(Y[Y==0].index)

            random_normal_indices = np.random.choice (indices_normal, no_frauds, replace = False).tolist()
            random_fraud_indices = np.random.choice(indices_fraud, no_frauds, replace = True).tolist()

            under_sample_indices = list((np.concatenate([random_normal_indices, random_fraud_indices])))

            X_train = np.array(X.loc[under_sample_indices])
            Y_train = np.array(Y.loc[under_sample_indices])

            clf = self.model
            clf.fit(X_train, Y_train)
            self.classifiers.append(clf)


    def predict_proba(self, X):

        predictions = []

        for X_temp in X:

            X_temp = list(X_temp)

            predict_temp = []

            for j in range(self.n_estimators):
                predict_temp.append(self.classifiers[j].predict_proba([X_temp])[0].tolist())

            predictions.append(np.sum(predict_temp, axis = 0).tolist())

        return np.array(predictions)

class model:
    def __init__ (self):
        self.models = [DecisionTreeClassifier(max_depth=7), KNN(n_neighbors = 5), RandomForestClassifier(n_estimators = 10, max_depth = 5)]
        self.Bag_models = []


    def fit(self, X, Y):

        # for i in range(20):

        for clf in self.models:
            temp_bag = BaggingClassifier(clf, n_estimators = 10)
            temp_bag.fit(X, Y)
            self.Bag_models.append(temp_bag)


    def predict(self, X):

        predictions = []

        for X_temp in X:

            X_temp = list(X_temp)

            predict_temp = []

            for temp_bag in self.Bag_models:

                predict_temp.append(temp_bag.predict_proba([X_temp])[0].tolist())

            predictions.append(np.sum(predict_temp, axis = 0).tolist())

        predictions = np.array(predictions)
        return np.where(predictions[:, 0] < predictions[:, 1], 1, 0)

class main_model:
    def __init__(self):
        self.models = []

    def fit(self, X, Y):

        self. LDA = LDA()
        self.LDA.fit(np.array(X), np.array(Y))

        for i in range(2, 20):
            X_trans = pd.DataFrame(self.LDA.transform(np.array(X), i))

            md = model()
            md.fit(X_trans, Y)

            self.models.append(md)

    def predict(self, X):

        predictions = []
        for i, md in enumerate(self.models):

            X_trans = self.LDA.transform(np.array(X), i+2)

            predictions.append(md.predict(X_trans))

        predictions = np.array(predictions)
        predictions = np.sum(predictions, axis = 0)
        # return predictions
        return np.where(predictions < 9, 0, 1)

"""Defining the Model and fitting Data into it."""

md = main_model()
md.fit(X, Y)
print(X_test.shape)

Y_pred = md.predict(np.array(X_test))

from sklearn.metrics import accuracy_score, f1_score
from sklearn.metrics import confusion_matrix

print("The F1_Score of the trained Model is :", f1_score(Y_pred, Y_test))
print("\nThe Accuracy Score of trained model is :", accuracy_score(Y_pred, Y_test))

print("\nThe Confusion Matrix is :\n", confusion_matrix(Y_pred, Y_test))

f1_scores = []
accuracy_scores = []


for i in range(5):

    random_indices = np.random.choice (X.shape[0], 100, replace = False )

    X_test = X.loc[random_indices, :]
    Y_test = Y.loc[random_indices]

    Y_pred = md.predict(np.array(X_test))

    accuracy_scores.append(accuracy_score(Y_pred, Y_test))

print("The Accuracies found are :", accuracy_scores)
print("The Mean Accuracy is :", np.sum(accuracy_scores)/len(accuracy_scores))